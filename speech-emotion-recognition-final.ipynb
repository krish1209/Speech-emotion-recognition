{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555}],"dockerImageVersionId":29928,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition <center>","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"!apt-get update\n!apt-get install -y libsndfile1","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-22T17:02:14.324701Z","iopub.execute_input":"2023-11-22T17:02:14.325110Z","iopub.status.idle":"2023-11-22T17:02:20.576264Z","shell.execute_reply.started":"2023-11-22T17:02:14.325078Z","shell.execute_reply":"2023-11-22T17:02:20.575157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-11-22T17:02:20.579034Z","iopub.execute_input":"2023-11-22T17:02:20.579379Z","iopub.status.idle":"2023-11-22T17:02:20.590133Z","shell.execute_reply.started":"2023-11-22T17:02:20.579346Z","shell.execute_reply":"2023-11-22T17:02:20.589017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.591320Z","iopub.execute_input":"2023-11-22T17:02:20.591687Z","iopub.status.idle":"2023-11-22T17:02:20.605364Z","shell.execute_reply.started":"2023-11-22T17:02:20.591657Z","shell.execute_reply":"2023-11-22T17:02:20.604567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 1. Ravdess Dataframe <center>","metadata":{}},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.606757Z","iopub.execute_input":"2023-11-22T17:02:20.607073Z","iopub.status.idle":"2023-11-22T17:02:20.662593Z","shell.execute_reply.started":"2023-11-22T17:02:20.607042Z","shell.execute_reply":"2023-11-22T17:02:20.661584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <center>2. Crema DataFrame</center>","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.665548Z","iopub.execute_input":"2023-11-22T17:02:20.665862Z","iopub.status.idle":"2023-11-22T17:02:20.701012Z","shell.execute_reply.started":"2023-11-22T17:02:20.665831Z","shell.execute_reply":"2023-11-22T17:02:20.700058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 3. TESS dataset <center>","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.704179Z","iopub.execute_input":"2023-11-22T17:02:20.704525Z","iopub.status.idle":"2023-11-22T17:02:20.737677Z","shell.execute_reply.started":"2023-11-22T17:02:20.704496Z","shell.execute_reply":"2023-11-22T17:02:20.736786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 4. SAVEE dataset <center>","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.738916Z","iopub.execute_input":"2023-11-22T17:02:20.739217Z","iopub.status.idle":"2023-11-22T17:02:20.764703Z","shell.execute_reply.started":"2023-11-22T17:02:20.739189Z","shell.execute_reply":"2023-11-22T17:02:20.763835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.766196Z","iopub.execute_input":"2023-11-22T17:02:20.766566Z","iopub.status.idle":"2023-11-22T17:02:20.847533Z","shell.execute_reply.started":"2023-11-22T17:02:20.766511Z","shell.execute_reply":"2023-11-22T17:02:20.846587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation and Exploration","metadata":{}},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(data_path.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:20.849219Z","iopub.execute_input":"2023-11-22T17:02:20.849608Z","iopub.status.idle":"2023-11-22T17:02:21.032322Z","shell.execute_reply.started":"2023-11-22T17:02:20.849568Z","shell.execute_reply":"2023-11-22T17:02:21.031467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:21.034188Z","iopub.execute_input":"2023-11-22T17:02:21.034663Z","iopub.status.idle":"2023-11-22T17:02:21.043264Z","shell.execute_reply.started":"2023-11-22T17:02:21.034618Z","shell.execute_reply":"2023-11-22T17:02:21.042343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='fear'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:21.044589Z","iopub.execute_input":"2023-11-22T17:02:21.044907Z","iopub.status.idle":"2023-11-22T17:02:21.685221Z","shell.execute_reply.started":"2023-11-22T17:02:21.044864Z","shell.execute_reply":"2023-11-22T17:02:21.684150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='angry'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:21.686679Z","iopub.execute_input":"2023-11-22T17:02:21.686967Z","iopub.status.idle":"2023-11-22T17:02:22.323976Z","shell.execute_reply.started":"2023-11-22T17:02:21.686938Z","shell.execute_reply":"2023-11-22T17:02:22.323008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='sad'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:22.325458Z","iopub.execute_input":"2023-11-22T17:02:22.325853Z","iopub.status.idle":"2023-11-22T17:02:22.959989Z","shell.execute_reply.started":"2023-11-22T17:02:22.325817Z","shell.execute_reply":"2023-11-22T17:02:22.958896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='happy'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:22.961682Z","iopub.execute_input":"2023-11-22T17:02:22.962083Z","iopub.status.idle":"2023-11-22T17:02:23.577007Z","shell.execute_reply.started":"2023-11-22T17:02:22.962036Z","shell.execute_reply":"2023-11-22T17:02:23.576236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n# taking any example and checking for techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:23.578465Z","iopub.execute_input":"2023-11-22T17:02:23.578863Z","iopub.status.idle":"2023-11-22T17:02:23.733123Z","shell.execute_reply.started":"2023-11-22T17:02:23.578823Z","shell.execute_reply":"2023-11-22T17:02:23.732197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Simple Audio","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:23.734700Z","iopub.execute_input":"2023-11-22T17:02:23.735034Z","iopub.status.idle":"2023-11-22T17:02:23.906634Z","shell.execute_reply.started":"2023-11-22T17:02:23.735003Z","shell.execute_reply":"2023-11-22T17:02:23.905772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Noise Injection","metadata":{}},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:23.908004Z","iopub.execute_input":"2023-11-22T17:02:23.908296Z","iopub.status.idle":"2023-11-22T17:02:24.085690Z","shell.execute_reply.started":"2023-11-22T17:02:23.908268Z","shell.execute_reply":"2023-11-22T17:02:24.084777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Stretching","metadata":{}},{"cell_type":"code","source":"x = stretch(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:24.087235Z","iopub.execute_input":"2023-11-22T17:02:24.087696Z","iopub.status.idle":"2023-11-22T17:02:24.309456Z","shell.execute_reply.started":"2023-11-22T17:02:24.087653Z","shell.execute_reply":"2023-11-22T17:02:24.308652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Shifting","metadata":{}},{"cell_type":"code","source":"x = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:24.310808Z","iopub.execute_input":"2023-11-22T17:02:24.311106Z","iopub.status.idle":"2023-11-22T17:02:24.482115Z","shell.execute_reply.started":"2023-11-22T17:02:24.311076Z","shell.execute_reply":"2023-11-22T17:02:24.481318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Pitch","metadata":{}},{"cell_type":"code","source":"x = pitch(data, sample_rate)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:24.483446Z","iopub.execute_input":"2023-11-22T17:02:24.483785Z","iopub.status.idle":"2023-11-22T17:02:24.755807Z","shell.execute_reply.started":"2023-11-22T17:02:24.483755Z","shell.execute_reply":"2023-11-22T17:02:24.754708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:24.757378Z","iopub.execute_input":"2023-11-22T17:02:24.757802Z","iopub.status.idle":"2023-11-22T17:02:24.775019Z","shell.execute_reply.started":"2023-11-22T17:02:24.757757Z","shell.execute_reply":"2023-11-22T17:02:24.773963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:02:24.776651Z","iopub.execute_input":"2023-11-22T17:02:24.777038Z","iopub.status.idle":"2023-11-22T17:02:24.791342Z","shell.execute_reply.started":"2023-11-22T17:02:24.777004Z","shell.execute_reply":"2023-11-22T17:02:24.790405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\ncount = 0\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    feature = get_features(path)\n    count+=1\n    print(count)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-22T17:02:24.792841Z","iopub.execute_input":"2023-11-22T17:02:24.793323Z","iopub.status.idle":"2023-11-22T17:49:18.543927Z","shell.execute_reply.started":"2023-11-22T17:02:24.793282Z","shell.execute_reply":"2023-11-22T17:49:18.542943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:18.545571Z","iopub.execute_input":"2023-11-22T17:49:18.546009Z","iopub.status.idle":"2023-11-22T17:49:18.553549Z","shell.execute_reply.started":"2023-11-22T17:49:18.545941Z","shell.execute_reply":"2023-11-22T17:49:18.552532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:18.554845Z","iopub.execute_input":"2023-11-22T17:49:18.555209Z","iopub.status.idle":"2023-11-22T17:49:36.703975Z","shell.execute_reply.started":"2023-11-22T17:49:18.555176Z","shell.execute_reply":"2023-11-22T17:49:36.703058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.705397Z","iopub.execute_input":"2023-11-22T17:49:36.705832Z","iopub.status.idle":"2023-11-22T17:49:36.741195Z","shell.execute_reply.started":"2023-11-22T17:49:36.705777Z","shell.execute_reply":"2023-11-22T17:49:36.740075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.742861Z","iopub.execute_input":"2023-11-22T17:49:36.743264Z","iopub.status.idle":"2023-11-22T17:49:36.774435Z","shell.execute_reply.started":"2023-11-22T17:49:36.743232Z","shell.execute_reply":"2023-11-22T17:49:36.773600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.775994Z","iopub.execute_input":"2023-11-22T17:49:36.776276Z","iopub.status.idle":"2023-11-22T17:49:36.841781Z","shell.execute_reply.started":"2023-11-22T17:49:36.776247Z","shell.execute_reply":"2023-11-22T17:49:36.840969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.842908Z","iopub.execute_input":"2023-11-22T17:49:36.843184Z","iopub.status.idle":"2023-11-22T17:49:36.957525Z","shell.execute_reply.started":"2023-11-22T17:49:36.843157Z","shell.execute_reply":"2023-11-22T17:49:36.956660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.959128Z","iopub.execute_input":"2023-11-22T17:49:36.959565Z","iopub.status.idle":"2023-11-22T17:49:36.967522Z","shell.execute_reply.started":"2023-11-22T17:49:36.959515Z","shell.execute_reply":"2023-11-22T17:49:36.966709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=8, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:36.969299Z","iopub.execute_input":"2023-11-22T17:49:36.969706Z","iopub.status.idle":"2023-11-22T17:49:37.118052Z","shell.execute_reply.started":"2023-11-22T17:49:36.969667Z","shell.execute_reply":"2023-11-22T17:49:37.117066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:49:37.124343Z","iopub.execute_input":"2023-11-22T17:49:37.124763Z","iopub.status.idle":"2023-11-22T17:53:03.799443Z","shell.execute_reply.started":"2023-11-22T17:49:37.124727Z","shell.execute_reply":"2023-11-22T17:53:03.798610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:53:03.801337Z","iopub.execute_input":"2023-11-22T17:53:03.801751Z","iopub.status.idle":"2023-11-22T17:53:04.927685Z","shell.execute_reply.started":"2023-11-22T17:53:03.801708Z","shell.execute_reply":"2023-11-22T17:53:04.926657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:59:00.793093Z","iopub.execute_input":"2023-11-22T17:59:00.793435Z","iopub.status.idle":"2023-11-22T17:59:01.503327Z","shell.execute_reply.started":"2023-11-22T17:59:00.793406Z","shell.execute_reply":"2023-11-22T17:59:01.502392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:59:24.479331Z","iopub.execute_input":"2023-11-22T17:59:24.479712Z","iopub.status.idle":"2023-11-22T17:59:24.486525Z","shell.execute_reply.started":"2023-11-22T17:59:24.479678Z","shell.execute_reply":"2023-11-22T17:59:24.485665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:15:45.332856Z","iopub.execute_input":"2023-11-22T18:15:45.333249Z","iopub.status.idle":"2023-11-22T18:15:45.899613Z","shell.execute_reply.started":"2023-11-22T18:15:45.333212Z","shell.execute_reply":"2023-11-22T18:15:45.898557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:15:58.776635Z","iopub.execute_input":"2023-11-22T18:15:58.777011Z","iopub.status.idle":"2023-11-22T18:15:58.795373Z","shell.execute_reply.started":"2023-11-22T18:15:58.776977Z","shell.execute_reply":"2023-11-22T18:15:58.794565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:16:02.212492Z","iopub.execute_input":"2023-11-22T18:16:02.212839Z","iopub.status.idle":"2023-11-22T18:16:02.762210Z","shell.execute_reply.started":"2023-11-22T18:16:02.212809Z","shell.execute_reply":"2023-11-22T18:16:02.761224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:16:10.174698Z","iopub.execute_input":"2023-11-22T18:16:10.175055Z","iopub.status.idle":"2023-11-22T18:16:10.617854Z","shell.execute_reply.started":"2023-11-22T18:16:10.175022Z","shell.execute_reply":"2023-11-22T18:16:10.616922Z"},"trusted":true},"execution_count":null,"outputs":[]}]}